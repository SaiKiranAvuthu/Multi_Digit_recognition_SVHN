{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "This notebook decribes converting the raw images and the bounding box into input to  go into my model to recogize the multi digit number.The data for the Multi digit recognition can be obtain from kaggle [here](https://www.kaggle.com/c/streetview-house-numbers-comp540-spr2017-term-project) or download the original dataset and follow the apporch to load.\n",
    "### SVHN Dataset\n",
    "SVHN dataset is a Street View House Number dataset consists of over 600000 images with the real world problems. This images is taken from the house numbers from the Google street view images.[Website](http://ufldl.stanford.edu/housenumbers/)\n",
    "### Preprocessing Steps\n",
    "1.The dataset consist of only one 6 digit number and the remaining data are 5 digits or less than 5 digits. So, removing the one 6 digit number and the 6 digit class will distribute the data evenly among remaining classes.\n",
    "2.We are croping the images to 30% in the x and y direction and resize to 32x32 pixel greyscale image to remove the unwanted data to process and reduce the training time.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiki\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import h5py\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from the website mention above and place the three tar files in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra already present - Skipping extraction of extra.tar.gz.\n",
      "test already present - Skipping extraction of test.tar.gz.\n",
      "train already present - Skipping extraction of train.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "def extract_tarball(filename, force=False):\n",
    "    \"\"\" Helper function for extracting tarball files\n",
    "    \"\"\"\n",
    "    # Drop the file extension\n",
    "    root = filename.split('.')[0] \n",
    "    \n",
    "    # If file is already extracted - return\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "        return\n",
    "    \n",
    "    # If file is a tarball file - extract it\n",
    "    if (filename.endswith(\"tar.gz\")):\n",
    "        print(\"Extracting %s ...\" % filename)\n",
    "        tar = tarfile.open(filename, \"r:gz\")\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "os.listdir(\"data\")\n",
    "# Get the directory listing for the dataset folder\n",
    "ls_data = [f for f in os.listdir(\"data\") if 'tar.gz' in f]\n",
    "        \n",
    "# cd data\n",
    "os.chdir(\"data\")\n",
    "\n",
    "# Extract the tarballs\n",
    "extract_tarball(ls_data[0])\n",
    "extract_tarball(ls_data[1])\n",
    "extract_tarball(ls_data[2])\n",
    "    \n",
    "# cd ..\n",
    "os.chdir(os.path.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the bounding box from the mat files using the unpacker.py file to extract the data and save into a json file for further uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ..........geting Train images bounding box\n",
      "Done ..........geting Test images bounding box\n",
      "Done ..........geting Extra images bounding box\n",
      "{\n",
      "  \"filename\": \"1.png\",\n",
      "  \"boxes\": [\n",
      "    {\n",
      "      \"height\": 219.0,\n",
      "      \"label\": 1.0,\n",
      "      \"left\": 246.0,\n",
      "      \"top\": 77.0,\n",
      "      \"width\": 81.0\n",
      "    },\n",
      "    {\n",
      "      \"height\": 219.0,\n",
      "      \"label\": 9.0,\n",
      "      \"left\": 323.0,\n",
      "      \"top\": 81.0,\n",
      "      \"width\": 96.0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from unpacker import DigitStructWrapper\n",
    "\n",
    "def get_bounding_boxes(start_path = '.'):\n",
    "    \"\"\" Extract\n",
    "s a bounding box file and returns a dictionary\n",
    "    \"\"\"\n",
    "    return DigitStructWrapper(start_path).unpack_all()\n",
    "\n",
    "# Extract the bounding boxes (this will take a while!)\n",
    "train_bbox = get_bounding_boxes('data/train/digitStruct.mat')\n",
    "print(\"Done ..........geting Train images bounding box\")\n",
    "test_bbox = get_bounding_boxes('data/test/digitStruct.mat')\n",
    "print(\"Done ..........geting Test images bounding box\")\n",
    "extra_bbox = get_bounding_boxes('data/extra/digitStruct.mat')\n",
    "print(\"Done ..........geting Extra images bounding box\")\n",
    "\n",
    "# Display the information stored about an individual image\n",
    "print(json.dumps(train_bbox[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the data and edit, we will move the data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiki\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:4658: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>labels</th>\n",
       "      <th>num_digits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/extra/1.png</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>[4.0, 7.0, 8.0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/extra/10.png</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[4.0, 4.0, 4.0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/extra/100.png</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>[5.0, 3.0, 5.0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/extra/1000.png</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[2.0, 6.0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/extra/10000.png</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>[2.0, 10.0, 10.0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename    x0    y0     x1     y1             labels  \\\n",
       "0      data/extra/1.png  24.0  23.0  126.0  126.0    [4.0, 7.0, 8.0]   \n",
       "1     data/extra/10.png   5.0   5.0   52.0   32.0    [4.0, 4.0, 4.0]   \n",
       "2    data/extra/100.png   3.0   1.0   46.0   30.0    [5.0, 3.0, 5.0]   \n",
       "3   data/extra/1000.png   9.0   7.0   36.0   40.0         [2.0, 6.0]   \n",
       "4  data/extra/10000.png   8.0   9.0   41.0   35.0  [2.0, 10.0, 10.0]   \n",
       "\n",
       "   num_digits  \n",
       "0           3  \n",
       "1           3  \n",
       "2           3  \n",
       "3           2  \n",
       "4           3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dict_to_dataframe(image_bounding_boxes, path):\n",
    "    \"\"\" Helper function for flattening the bounding box dictionary\n",
    "    \"\"\"\n",
    "    # Store each bounding box\n",
    "    boxes = []\n",
    "    \n",
    "    # For each set of bounding boxes\n",
    "    for image in tqdm(image_bounding_boxes):\n",
    "        \n",
    "        # For every bounding box\n",
    "        for bbox in image['boxes']:\n",
    "            \n",
    "            # Store a dict with the file and bounding box info\n",
    "            boxes.append({\n",
    "                    'filename': path + image['filename'],\n",
    "                    'label': bbox['label'],\n",
    "                    'width': bbox['width'],\n",
    "                    'height': bbox['height'],\n",
    "                    'top': bbox['top'],\n",
    "                    'left': bbox['left']})\n",
    "            \n",
    "    # return the data as a DataFrame\n",
    "    return pd.DataFrame(boxes)\n",
    "\n",
    "\n",
    "# We store the bounding boxes here\n",
    "bbox_file = 'data/bounding_boxes.csv'\n",
    "\n",
    "if not os.path.isfile(bbox_file):\n",
    "    \n",
    "    # Extract every individual bounding box as DataFrame  \n",
    "    train_df = dict_to_dataframe(train_bbox, 'data/train/')\n",
    "    test_df = dict_to_dataframe(test_bbox, 'data/test/')\n",
    "    extra_df = dict_to_dataframe(extra_bbox, 'data/extra/')\n",
    "\n",
    "    print(\"Training\", train_df.shape)\n",
    "    print(\"Test\", test_df.shape)\n",
    "    print(\"Extra\", extra_df.shape)\n",
    "    print('')\n",
    "\n",
    "    # Concatenate all the information in a single file\n",
    "    df = pd.concat([train_df, test_df, extra_df])\n",
    "    \n",
    "    print(\"Combined\", df.shape)\n",
    "\n",
    "    # Write dataframe to csv\n",
    "    df.to_csv(bbox_file, index=False)\n",
    "\n",
    "    # Delete the old dataframes\n",
    "    del train_df, test_df, extra_df, train_bbox, test_bbox, extra_bbox\n",
    "    \n",
    "else:\n",
    "    # Load preprocessed bounding boxes\n",
    "    df = pd.read_csv(bbox_file)\n",
    "\n",
    "\n",
    "# Rename the columns to more suitable names\n",
    "df.rename(columns={'left': 'x0', 'top': 'y0'}, inplace=True)\n",
    "\n",
    "# Calculate x1 and y1\n",
    "df['x1'] = df['x0'] + df['width']\n",
    "df['y1'] = df['y0'] + df['height']\n",
    "\n",
    "# Perform the following aggregations\n",
    "aggregate = {'x0':'min',\n",
    "             'y0':'min',\n",
    "             'x1':'max',\n",
    "             'y1':'max',\n",
    "             'label':{\n",
    "                'labels': lambda x: list(x),\n",
    "                'num_digits': 'count'}}\n",
    "\n",
    "# Apply the aggration\n",
    "df = df.groupby('filename').agg(aggregate).reset_index()\n",
    "\n",
    "# Fix the column names after aggregation\n",
    "df.columns = [x[0] if i < 5 else x[1] for i, x in enumerate(df.columns.values)]\n",
    "\n",
    "# Display the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding box is increased 30% to its dimensions for better image to crop and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the increase in both directions\n",
    "df['x_increase'] = ((df['x1'] - df['x0']) * 0.3) / 2.\n",
    "df['y_increase'] = ((df['y1'] - df['y0']) * 0.3) / 2.\n",
    "\n",
    "# Apply the increase in all four directions\n",
    "df['x0'] = (df['x0'] - df['x_increase']).astype('int')\n",
    "df['y0'] = (df['y0'] - df['y_increase']).astype('int')\n",
    "df['x1'] = (df['x1'] + df['x_increase']).astype('int')\n",
    "df['y1'] = (df['y1'] + df['y_increase']).astype('int')\n",
    "\n",
    "\n",
    "# Select the dataframe row corresponding to our image\n",
    "image = 'data/train/1.png'\n",
    "bbox = df[df.filename == image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filesize is taken and added into the pd while adjusting the boxes which are out of limits of photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size(filepath):\n",
    "    \"\"\"Returns the image size in pixels given as a 2-tuple (width, height)\n",
    "    \"\"\"\n",
    "    image = Image.open(filepath)\n",
    "    return image.size \n",
    "\n",
    "def get_image_sizes(folder):\n",
    "    \"\"\"Returns a DataFrame with the file name and size of all images contained in a folder\n",
    "    \"\"\"\n",
    "    image_sizes = []\n",
    "    \n",
    "    # Get all .png images contained in the folder\n",
    "    images = [img for img in os.listdir(folder) if img.endswith('.png')]\n",
    "    \n",
    "    # Get image size of every individual image\n",
    "    for i,image in enumerate(images):\n",
    "        w, h = get_image_size(folder + image)\n",
    "        image_size = {'filename': folder + image, 'image_width': w, 'image_height': h}\n",
    "        image_sizes.append(image_size)\n",
    "        if i%10000==0: \n",
    "            print(i)\n",
    "        \n",
    "    # Return results as a pandas DataFrame\n",
    "    return pd.DataFrame(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the image sizes\n",
    "train_sizes = get_image_sizes('data/train/')\n",
    "print(\"Done............getting Train_Set sizes\")\n",
    "test_sizes = get_image_sizes('data/test/')\n",
    "print(\"Done............getting Test_Set sizes\")\n",
    "extra_sizes = get_image_sizes('data/extra/')\n",
    "print(\"Done............getting extra_Set sizes\")\n",
    "\n",
    "# Concatenate all the information in a single file\n",
    "image_sizes = pd.concat([train_sizes, test_sizes, extra_sizes])\n",
    "\n",
    "# Delete old dataframes\n",
    "del train_sizes, test_sizes, extra_sizes\n",
    "\n",
    "print(\"Bounding boxes\", df.shape)\n",
    "print(\"Image sizes\", image_sizes.shape)\n",
    "print('')\n",
    "\n",
    "# Inner join the datasets on filename\n",
    "df = pd.merge(df, image_sizes, on='filename', how='inner')\n",
    "\n",
    "print(\"Combined\", df.shape)\n",
    "\n",
    "# Delete the image size df\n",
    "del image_sizes\n",
    "\n",
    "# Store checkpoint\n",
    "df.to_csv(\"data/image_data.csv\", index=False)\n",
    "#df = pd.read_csv('data/image_data.csv')\n",
    "\n",
    "# Correct bounding boxes not contained by image\n",
    "df.loc[df['x0'] < 0, 'x0'] = 0\n",
    "df.loc[df['y0'] < 0, 'y0'] = 0\n",
    "df.loc[df['x1'] > df['image_width'], 'x1'] = df['image_width']\n",
    "df.loc[df['y1'] > df['image_height'], 'y1'] = df['image_width']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the 6 digit number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of images by number of digits\n",
    "df.num_digits.value_counts(sort=False)\n",
    "# Keep only images with less than 6 digits\n",
    "df = df[df.num_digits < 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the images using the bounding boxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imresize\n",
    "\n",
    "def crop_and_resize(image, img_size):\n",
    "    \"\"\" Crop and resize an image\n",
    "    \"\"\"\n",
    "    image_data = imread(image['filename'])\n",
    "    crop = image_data[image['y0']:image['y1'], image['x0']:image['x1'], :]\n",
    "    return imresize(crop, img_size)\n",
    "\n",
    "\n",
    "def create_dataset(df, img_size):\n",
    "    \"\"\" Helper function for converting images into a numpy array\n",
    "    \"\"\"\n",
    "    # Initialize the numpy arrays (0's are stored as 10's)\n",
    "    X = np.zeros(shape=(df.shape[0], img_size[0], img_size[0], 3), dtype='uint8')\n",
    "    y = np.full((df.shape[0], 5), 10, dtype=int)\n",
    "    \n",
    "    # Iterate over all images in the pandas dataframe (slow!)\n",
    "    for i, (index, image) in enumerate(df.iterrows()):\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        # Get the image data\n",
    "        X[i] = crop_and_resize(image, img_size)\n",
    "        \n",
    "        # Get the label list as an array\n",
    "        labels = np.array((image['labels']))\n",
    "                \n",
    "        # Store 0's as 0 (not 10)\n",
    "        labels[labels==10] = 0\n",
    "        \n",
    "        # Embed labels into label array\n",
    "        y[i,0:labels.shape[0]] = labels\n",
    "        \n",
    "    # Return data and labels   \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Change this to select a different image size\n",
    "image_size = (32, 32)\n",
    "\n",
    "# Get cropped images and labels (this might take a while...)\n",
    "X_train, y_train = create_dataset(df[df.filename.str.contains('train')], image_size)\n",
    "print(\"Done Dataset ...............Train data\")\n",
    "X_test, y_test = create_dataset(df[df.filename.str.contains('test')], image_size)\n",
    "print(\"Done Dataset ...............Test data\")\n",
    "X_extra, y_extra = create_dataset(df[df.filename.str.contains('extra')], image_size)\n",
    "print(\"Done Dataset ...............extra data\")\n",
    "# We no longer need the dataframe\n",
    "del df\n",
    "\n",
    "print(\"Training\", X_train.shape, y_train.shape)\n",
    "print(\"Test\", X_test.shape, y_test.shape)\n",
    "print('Extra', X_extra.shape, y_extra.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize the data for better learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (230754, 32, 32, 3) (230754, 5)\n",
      "Validation (5000, 32, 32, 3) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "def random_sample(N, K):\n",
    "    \"\"\"Return a boolean mask of size N with K selections\n",
    "    \"\"\"\n",
    "    mask = np.array([True]*K + [False]*(N-K))\n",
    "    np.random.shuffle(mask)\n",
    "    return mask\n",
    "\n",
    "# Pick 4000 training and 2000 extra samples\n",
    "sample1 = random_sample(X_train.shape[0], 3000)\n",
    "sample2 = random_sample(X_extra.shape[0], 2000)\n",
    "\n",
    "# Create valdidation from the sampled data\n",
    "X_val = np.concatenate([X_train[sample1], X_extra[sample2]])\n",
    "y_val = np.concatenate([y_train[sample1], y_extra[sample2]])\n",
    "\n",
    "# Keep the data not contained by sample\n",
    "X_train = np.concatenate([X_train[~sample1], X_extra[~sample2]])\n",
    "y_train = np.concatenate([y_train[~sample1], y_extra[~sample2]])\n",
    "\n",
    "# Moved to validation and training set\n",
    "del X_extra, y_extra \n",
    "\n",
    "print(\"Training\", X_train.shape, y_train.shape)\n",
    "print('Validation', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into greyscale and add into h5 file for the model to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(images):\n",
    "    \"\"\"Convert images from rbg to grayscale\n",
    "    \"\"\"\n",
    "    greyscale = np.dot(images, [0.2989, 0.5870, 0.1140])\n",
    "    return np.expand_dims(greyscale, axis=3)\n",
    "\n",
    "\n",
    "# Transform the images to greyscale\n",
    "X_train = rgb2gray(X_train).astype(np.float32)\n",
    "X_test = rgb2gray(X_test).astype(np.float32)\n",
    "X_val = rgb2gray(X_val).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file\n",
    "h5f = h5py.File('data/SVHN_multi_grey.h5', 'w')\n",
    "\n",
    "# Store the datasets\n",
    "h5f.create_dataset('train_dataset', data=X_train)\n",
    "h5f.create_dataset('train_labels', data=y_train)\n",
    "h5f.create_dataset('test_dataset', data=X_test)\n",
    "h5f.create_dataset('test_labels', data=y_test)\n",
    "h5f.create_dataset('valid_dataset', data=X_val)\n",
    "h5f.create_dataset('valid_labels', data=y_val)\n",
    "\n",
    "# Close the file\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
